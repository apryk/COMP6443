# Tutorial 3

Group report due 7 hours after challenge 3.

Flags don't matter, how you found and exploited the site is important.

## robots.txt

Tells what files you can and can't scrape

`User-agent: *` - any bot

Once you put a file in robots.txt, a normal user can still access it. Only bots aren't allowed.

We use encoding for compatibility. Some protocols may interpret characters specially, so encoding can help avoid this.

Tips for topic 2 challenges

* Need to brute force (but not subdirectory or subdomains)
* Don't stress too much on techniques, find out what info is available (find endpoints and attack points)

## Report

Finding a flag = finding a bug for this report
